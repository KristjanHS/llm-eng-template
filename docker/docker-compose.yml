name: llm-eng-template
services:

  # ---------- Local LLM server ---------------------------------------------------
  ollama:
    image: ${OLLAMA_IMAGE:-ollama/ollama:latest}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ../model_cache:/root/.ollama
    ports:
      - "127.0.0.1:11434:11434"
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s

  # ---------- App Container -----------------------------------------
  app:
    image: app
    build:
      context: ..
      dockerfile: docker/app.Dockerfile
      target: runtime
    volumes:
      - ../backend:/app/backend # Live backend code
      - ../frontend:/app/frontend # Live frontend code
      - ../logs:/app/logs # Live log access
      - ../tests:/app/tests # Mount tests for in-container testing
      - ../scripts:/app/scripts # Shell helpers used by tests
      - ../docker:/app/docker # Compose file presence checked by scripts/common.sh
      - ../reports:/app/reports # Persist test reports to host
    working_dir: /app
    # Start minimal backend keepalive entrypoint
    command: ["python", "-m", "backend.main"]
    healthcheck:
      test: ["CMD", "python", "-m", "backend.main", "--healthcheck"]
      interval: 5s
      timeout: 3s
      retries: 30
      start_period: 30s
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      # Point the Python code to the other services inside the compose network
      - OLLAMA_URL=http://ollama:11434
      - APP_LOG_DIR=/app/logs
